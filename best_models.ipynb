{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import glob\r\n",
    "from datetime import datetime\r\n",
    "import os\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from scipy import stats, signal\r\n",
    "from scipy.stats import mode\r\n",
    "from scipy.fft import fft\r\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, ExtraTreesClassifier as ETC\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, StratifiedKFold, train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "\r\n",
    "\r\n",
    "import lightgbm as lgb\r\n",
    "import xgboost as xgb\r\n",
    "\r\n",
    "from tqdm import tqdm, trange"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generic functions (usable for all test cases)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Core functions (Change path here)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    ##### CHANGE PATH ######\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    ##### ##### ##### ######\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def cv_10_fold(model, X, y, n_repeats=10, verbose=True):\r\n",
    "    scores = []\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        X_data = X[1] + X[2] + X[3]\r\n",
    "        y_data = y[1] + y[2] + y[3]\r\n",
    "        X_data = np.array(X_data)\r\n",
    "        y_data = np.array(y_data)\r\n",
    "        scaler = MinMaxScaler()\r\n",
    "        X_data_scaled = scaler.fit_transform(X_data)\r\n",
    "        # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=False)\r\n",
    "        n_scores = cross_val_score(model, X_data, y_data, scoring='accuracy', cv=cv, \r\n",
    "                            n_jobs=-1, error_score='raise')\r\n",
    "        scores.extend(n_scores)\r\n",
    "    if verbose:\r\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\r\n",
    "        print(f\"Std Score: {np.std(scores)}\")\r\n",
    "        print(f\"Min Score: {np.min(scores)}\")\r\n",
    "        print(f\"Max Score: {np.max(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_processed_dataset(overlap_rate, window_size, verbose=True):\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    return X, y, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True):\r\n",
    "    scores = []\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\r\n",
    "        print(f\"Std Score: {np.std(scores)}\")\r\n",
    "        print(f\"Min Score: {np.min(scores)}\")\r\n",
    "        print(f\"Max Score: {np.max(scores)}\")\r\n",
    "    return scores\r\n",
    "\r\n",
    "\r\n",
    "def scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True):\r\n",
    "    scores = []\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            scaler = MinMaxScaler()\r\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\r\n",
    "            X_test_scaled = scaler.fit_transform(X_test)\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train_scaled, y_train)\r\n",
    "            pred = model.predict(X_test_scaled)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\r\n",
    "        print(f\"Std Score: {np.std(scores)}\")\r\n",
    "        print(f\"Min Score: {np.min(scores)}\")\r\n",
    "        print(f\"Max Score: {np.max(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stream Extractors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_speed_acc_jerk(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    jerk = acc.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    jerk.columns = [f\"{col}_jerk\" for col in acc.columns]\r\n",
    "    return speed, acc, jerk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def joint_angle(x_data, joint1, joint2, joint3):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    x3, y3, z3 = x_data[f\"{joint3}_X\"], x_data[f\"{joint3}_Y\"], x_data[f\"{joint3}_Z\"]\r\n",
    "    v1 = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2]).T\r\n",
    "    v1_unit = v1/np.expand_dims(np.linalg.norm(v1, axis=1), axis=1)\r\n",
    "    v2_unit = v2/np.expand_dims(np.linalg.norm(v2, axis=1), axis=1)\r\n",
    "    angle = np.arccos(np.sum(v1_unit*v2_unit, axis=1)) # dot multiplication\r\n",
    "    return angle\r\n",
    "\r\n",
    "def get_all_joint_angles(x_data):\r\n",
    "    # joints\r\n",
    "    # left shoulder     ->  left elbow      ->  left wrist      (8->9->10)\r\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\r\n",
    "    # right shoulder    ->  right elbow     ->  right wrist     (4->6->7)\r\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  front head      (4->8->1)\r\n",
    "    x_data[\"angle_RS_LS_FH\"] = joint_angle(x_data, \"RS\", \"LS\", \"FH\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  left elbow      (4->8->9)\r\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\r\n",
    "    # left shoulder     ->  right shoulder  ->  right elbow     (8->4->6)\r\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\r\n",
    "    # v sacral          ->  right offset    ->  rear head       (13->5->3)\r\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\r\n",
    "    # vsacral           ->  top head        ->  front head      (13->2->1)\r\n",
    "    x_data[\"angle_VS_TH_FH\"] = joint_angle(x_data, \"VS\", \"TH\", \"FH\")\r\n",
    "    # v sacral          ->  left shoulder   ->  left elbow      (13->8->9)\r\n",
    "    x_data[\"angle_VS_LS_LE\"] = joint_angle(x_data, \"VS\", \"LS\", \"LE\")\r\n",
    "    # v sacral          ->  right shoulder  ->  right elbow     (13->4->6)\r\n",
    "    x_data[\"angle_VS_RS_RE\"] = joint_angle(x_data, \"VS\", \"RS\", \"RE\")\r\n",
    "    # left asis         ->  left shoulder   ->  left elbow      (12->8->9)\r\n",
    "    x_data[\"angle_LA_LS_LE\"] = joint_angle(x_data, \"LA\", \"LS\", \"LE\")\r\n",
    "    # right asis        -> right shoulder   ->  right elbow     (11->4->6)\r\n",
    "    x_data[\"angle_RA_RS_RE\"] = joint_angle(x_data, \"RA\", \"RS\", \"RE\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_all_joint_angles_reduced(x_data):\r\n",
    "    # joints\r\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\r\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\r\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\r\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\r\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_all_joint_angles_diff_reduced(x_data):\r\n",
    "    # joints\r\n",
    "    x_data[\"ang_dif_LS_LE_LW\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"LS\", \"LE\", \"LW\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_RS_RE_RW\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"RS\", \"RE\", \"RW\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_RS_LS_LE\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"RS\", \"LS\", \"LE\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_LS_RS_RE\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"LS\", \"RS\", \"RE\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_VS_RO_RH\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"VS\", \"RO\", \"RH\")), 0)).tolist()\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def get_all_joint_angles_diff_red(x_data):\r\n",
    "    # joints\r\n",
    "    x_data[\"ang_dif_LS_LE_LW\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"LS\", \"LE\", \"LW\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_RS_RE_RW\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"RS\", \"RE\", \"RW\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_RS_LS_LE\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"RS\", \"LS\", \"LE\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_LS_RS_RE\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"LS\", \"RS\", \"RE\")), 0)).tolist()\r\n",
    "    x_data[\"ang_dif_VS_RO_RH\"] = np.nan_to_num(np.append(np.diff(joint_angle(x_data, \"VS\", \"RO\", \"RH\")), 0)).tolist()\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processed functions (add/remove streams/features here)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def current_best():\r\n",
    "    def get_streams(x_data):\r\n",
    "        speed, acc = get_speed_acc(x_data)\r\n",
    "        x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "        # speed, acc, jerk = get_speed_acc_jerk(x_data)\r\n",
    "        # x_data = pd.concat([x_data, speed, acc, jerk], axis=1)\r\n",
    "        x_data = get_all_joint_distances(x_data)\r\n",
    "        # x_data = get_all_joint_angles(x_data)\r\n",
    "        # x_data = get_all_angles_with_plane(x_data)\r\n",
    "        return x_data\r\n",
    "\r\n",
    "    def get_features(x_data):\r\n",
    "        features = []\r\n",
    "        cols = x_data.columns.tolist()\r\n",
    "        #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "        for k in cols:\r\n",
    "            features.append(x_data[k].std(ddof=0))\r\n",
    "            features.append(np.max(x_data[k]))\r\n",
    "            features.append(np.min(x_data[k]))\r\n",
    "            features.append(np.median(x_data[k]))        \r\n",
    "        return features\r\n",
    "    joint_names = ['FH', 'TH', 'RH', 'RS', 'RO', 'RE', 'RW', 'LS', 'LE', 'LW', 'RA', 'LA', 'VS']\r\n",
    "    pos_cols = [i+j for i in joint_names for j in [\"_X\", \"_Y\", \"_Z\"]]\r\n",
    "    speed_cols = [f\"{col}_speed\" for col in pos_cols]\r\n",
    "    acc_cols = [f\"{col}_acc\" for col in pos_cols]\r\n",
    "    distance_cols = ['dist_FH_LS', 'dist_FH_RS', 'dist_LS_LW', 'dist_RS_RW', 'dist_VS_LE', 'dist_VS_RE', \r\n",
    "        'dist_VS_LW', 'dist_VS_RW','dist_VS_RH', 'dist_VS_TH', 'dist_LW_RW', 'dist_LA_LW', 'dist_RA_RW', \r\n",
    "        'dist_LW_TH', 'dist_RW_TH', 'dist_TH_LA']\r\n",
    "    colnames = pos_cols + speed_cols + acc_cols + distance_cols\r\n",
    "    return get_streams, get_features, colnames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def remove_acc():\r\n",
    "    def get_streams(x_data):\r\n",
    "        speed, acc = get_speed_acc(x_data)\r\n",
    "        x_data = pd.concat([x_data, speed], axis=1)\r\n",
    "        x_data = get_all_joint_distances(x_data)\r\n",
    "        return x_data\r\n",
    "\r\n",
    "    def get_features(x_data):\r\n",
    "        features = []\r\n",
    "        cols = x_data.columns.tolist()\r\n",
    "        for k in cols:\r\n",
    "            features.append(x_data[k].std(ddof=0))\r\n",
    "            features.append(np.max(x_data[k]))\r\n",
    "            features.append(np.min(x_data[k]))\r\n",
    "            features.append(np.median(x_data[k]))        \r\n",
    "        return features\r\n",
    "    joint_names = ['FH', 'TH', 'RH', 'RS', 'RO', 'RE', 'RW', 'LS', 'LE', 'LW', 'RA', 'LA', 'VS']\r\n",
    "    pos_cols = [i+j for i in joint_names for j in [\"_X\", \"_Y\", \"_Z\"]]\r\n",
    "    speed_cols = [f\"{col}_speed\" for col in pos_cols]\r\n",
    "    distance_cols = ['dist_FH_LS', 'dist_FH_RS', 'dist_LS_LW', 'dist_RS_RW', 'dist_VS_LE', 'dist_VS_RE', \r\n",
    "        'dist_VS_LW', 'dist_VS_RW','dist_VS_RH', 'dist_VS_TH', 'dist_LW_RW', 'dist_LA_LW', 'dist_RA_RW', \r\n",
    "        'dist_LW_TH', 'dist_RW_TH', 'dist_TH_LA']\r\n",
    "    colnames = pos_cols + speed_cols + distance_cols\r\n",
    "    return get_streams, get_features, colnames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def angle_red_remove_acc():\r\n",
    "    def get_streams(x_data):\r\n",
    "        speed, acc = get_speed_acc(x_data)\r\n",
    "        x_data = pd.concat([x_data, speed], axis=1)\r\n",
    "        x_data = get_all_joint_distances(x_data)\r\n",
    "        x_data = get_all_joint_angles_diff_red(x_data)\r\n",
    "        return x_data\r\n",
    "\r\n",
    "    def get_features(x_data):\r\n",
    "        features = []\r\n",
    "        cols = x_data.columns.tolist()\r\n",
    "        for k in cols:\r\n",
    "            features.append(x_data[k].std(ddof=0))\r\n",
    "            features.append(np.max(x_data[k]))\r\n",
    "            features.append(np.min(x_data[k]))\r\n",
    "            features.append(np.median(x_data[k]))        \r\n",
    "        return features\r\n",
    "    joint_names = ['FH', 'TH', 'RH', 'RS', 'RO', 'RE', 'RW', 'LS', 'LE', 'LW', 'RA', 'LA', 'VS']\r\n",
    "    pos_cols = [i+j for i in joint_names for j in [\"_X\", \"_Y\", \"_Z\"]]\r\n",
    "    speed_cols = [f\"{col}_speed\" for col in pos_cols]\r\n",
    "    distance_cols = ['dist_FH_LS', 'dist_FH_RS', 'dist_LS_LW', 'dist_RS_RW', 'dist_VS_LE', 'dist_VS_RE', \r\n",
    "        'dist_VS_LW', 'dist_VS_RW','dist_VS_RH', 'dist_VS_TH', 'dist_LW_RW', 'dist_LA_LW', 'dist_RA_RW', \r\n",
    "        'dist_LW_TH', 'dist_RW_TH', 'dist_TH_LA']\r\n",
    "    angle_dif_cols = [\"LSLELW\", \"RSRERW\", \"RSLSLE\", \"LSRSRE\", \"VSRORH\"]\r\n",
    "    colnames = pos_cols + speed_cols + distance_cols + angle_dif_cols\r\n",
    "    return get_streams, get_features, colnames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1: RFC raw"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "get_streams, get_features, colnames = remove_acc()\r\n",
    "overlap_rate, window_size = 0.8, 4000\r\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:12<00:00, 12.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 512/512 [00:41<00:00, 12.48it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "model = RFC(2500, criterion=\"gini\", n_jobs=-1)\r\n",
    "scores = scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [03:52<00:00, 23.21s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Score: 0.76429487594862\n",
      "Std Score: 0.022429624437675363\n",
      "Min Score: 0.7293233082706767\n",
      "Max Score: 0.7973856209150327\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2: RFC with angle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "get_streams, get_features, colnames = angle_red_remove_acc()\r\n",
    "overlap_rate, window_size = 0.8, 4000\r\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:14<00:00, 10.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 512/512 [00:42<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "model = RFC(3000, criterion=\"gini\", n_jobs=-1)\r\n",
    "scores = scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [04:42<00:00, 28.25s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Score: 0.7749741132285861\n",
      "Std Score: 0.0164760926435766\n",
      "Min Score: 0.7443609022556391\n",
      "Max Score: 0.8120300751879699\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 3: ETC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "get_streams, get_features, colnames = remove_acc()\r\n",
    "overlap_rate, window_size = 0.75, 4000\r\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)\r\n",
    "ft_names = [i+j for i in colnames for j in [\"_std\", \"_max\", \"_min\", \"_med\"]]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:15<00:00,  9.85it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:31<00:00, 12.96it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "model = ETC(2400, criterion=\"entropy\", max_depth=12, min_samples_split=4, n_jobs=-1)\r\n",
    "scores = scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [02:45<00:00, 16.54s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Score: 0.7512445423040776\n",
      "Std Score: 0.021810399301761766\n",
      "Min Score: 0.7102803738317757\n",
      "Max Score: 0.7932960893854749\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def get_mode(file_pred):\r\n",
    "    modes = mode(file_pred)\r\n",
    "    majority_choice = modes.mode[0]\r\n",
    "    return majority_choice"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def model_ensembler(model1, model2, model3, X, y, file_lengths, n_repeats=10, voting=True, verbose=True):\r\n",
    "    scores = []\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            scaler = MinMaxScaler()\r\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\r\n",
    "            X_test_scaled = scaler.fit_transform(X_test)\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            \r\n",
    "            model1.fit(X_train_scaled, y_train)\r\n",
    "            pred1 = model1.predict(X_test_scaled)\r\n",
    "            \r\n",
    "            model2.fit(X_train_scaled, y_train)\r\n",
    "            pred2 = model2.predict(X_test_scaled)\r\n",
    "\r\n",
    "            model3.fit(X_train_scaled, y_train)\r\n",
    "            pred3 = model3.predict(X_test_scaled)\r\n",
    "            voting = True\r\n",
    "            filtered_pred1 = majority_voting(pred1, file_lengths[p1])\r\n",
    "            filtered_pred2 = majority_voting(pred2, file_lengths[p1])\r\n",
    "            filtered_pred3 = majority_voting(pred3, file_lengths[p1])\r\n",
    "            ensembled_pred = []\r\n",
    "            for i in zip(pred1, pred2, pred3):\r\n",
    "                ensembled_pred.append(get_mode(list(i)))\r\n",
    "            scores.append(accuracy_score(y_test, ensembled_pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\r\n",
    "        print(f\"Std Score: {np.std(scores)}\")\r\n",
    "        print(f\"Min Score: {np.min(scores)}\")\r\n",
    "        print(f\"Max Score: {np.max(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "get_streams, get_features, colnames = angle_red_remove_acc()\r\n",
    "overlap_rate, window_size = 0.8, 4000\r\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:13<00:00, 11.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 512/512 [00:48<00:00, 10.50it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model1 = RFC(2500, criterion=\"gini\", n_jobs=-1)\r\n",
    "model2 = RFC(3000, criterion=\"gini\", n_jobs=-1)\r\n",
    "model3 = ETC(2400, criterion=\"entropy\", max_depth=12, min_samples_split=4, n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "model_ensembler(model1, model2, model3, X, y, file_lengths, n_repeats=5, voting=True, verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [05:35<00:00, 67.01s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Score: 0.7304229687527859\n",
      "Std Score: 0.017621510972266344\n",
      "Min Score: 0.6993464052287581\n",
      "Max Score: 0.7593984962406015\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6993464052287581,\n",
       " 0.7212389380530974,\n",
       " 0.7443609022556391,\n",
       " 0.7189542483660131,\n",
       " 0.7212389380530974,\n",
       " 0.7518796992481203,\n",
       " 0.7058823529411765,\n",
       " 0.7168141592920354,\n",
       " 0.7518796992481203,\n",
       " 0.7189542483660131,\n",
       " 0.7345132743362832,\n",
       " 0.7593984962406015,\n",
       " 0.7254901960784313,\n",
       " 0.7345132743362832,\n",
       " 0.7518796992481203]"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "cv_10_fold(model2, X, y, n_repeats=1, verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [01:18<00:00, 78.17s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Score: 0.9631221719457013\n",
      "Std Score: 0.03596652603206732\n",
      "Min Score: 0.8846153846153846\n",
      "Max Score: 1.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.8846153846153846,\n",
       " 0.9230769230769231,\n",
       " 0.9607843137254902,\n",
       " 1.0,\n",
       " 0.9803921568627451,\n",
       " 0.9803921568627451,\n",
       " 0.9607843137254902,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9411764705882353]"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}