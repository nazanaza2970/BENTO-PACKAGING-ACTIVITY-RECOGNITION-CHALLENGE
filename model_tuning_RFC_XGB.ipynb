{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats, signal\n",
    "from scipy.stats import mode\n",
    "from scipy.fft import fft\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, ExtraTreesClassifier as ETC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic functions (usable for all test cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(df, overlap_rate, time_window):\n",
    "    seg_data = []\n",
    "    #convert overlap rate to step for sliding window\n",
    "    overlap = int((1 - overlap_rate)*time_window)\n",
    "    # interpolate\n",
    "    df = df.interpolate().ffill().fillna(0)\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\n",
    "    return seg_data\n",
    "\n",
    "\n",
    "def rename_columns(df):\n",
    "    df.columns = [\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\n",
    "        \"subject_id\", \"activity\",   # Other columns\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def dataloader(overlap, window_size, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"loading the data...\", end=\"\\t\")\n",
    "    data_list = []\n",
    "    file_lengths = {1: [], 2: [], 3: []}\n",
    "    files = tqdm(glob.glob(\"E:/Temp/Bento/Data/TrainData/*/*/*.csv\")) if verbose else glob.glob(\"E:/Temp/Bento/Data/TrainData/*/*/*.csv\")\n",
    "    for file in files:\n",
    "        tempdf = pd.read_csv(file)\n",
    "        tempdf = rename_columns(tempdf)\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\n",
    "        if len(segmented_data)>0:\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\n",
    "            file_lengths[person].append(len(segmented_data))   \n",
    "        data_list.extend(segmented_data)\n",
    "    return data_list, file_lengths\n",
    "\n",
    "\n",
    "def feature_extractor(data_list, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"extracting the features...\", end=\"  \")\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\n",
    "    for j in num_range:\n",
    "        #extract only xyz columns\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\n",
    "        X[person].append(get_features(x_data))\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def majority_voting(predictions, file_lengths):\n",
    "    filtered_predictions = []\n",
    "    index = 0\n",
    "    for length in file_lengths:\n",
    "        file_pred = predictions[index:index+length]\n",
    "        modes = mode(file_pred)\n",
    "        majority_choice = modes.mode[0]\n",
    "        filtered_predictions.extend([majority_choice]*length)\n",
    "        index += length\n",
    "    return filtered_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_dataset(overlap_rate, window_size, verbose=True):\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\n",
    "    stream_list = []\n",
    "    for df in data_list:\n",
    "        stream_list.append(get_streams(df))\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\n",
    "    return X, y, file_lengths\n",
    "\n",
    "\n",
    "def model_evaluator(model, X, y, file_lengths, n_repeats=7, voting=True, verbose=True):\n",
    "    scores = []\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\n",
    "    for _ in num_range:\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\n",
    "            X_test, y_test = X[p1], y[p1]\n",
    "            X_train = X[p2] + X[p3]\n",
    "            y_train = y[p2] + y[p3]\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            if voting:\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\n",
    "            else:\n",
    "                scores.append(accuracy_score(y_test, pred))\n",
    "    if verbose:\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\n",
    "        print(f\"Std Score: {np.std(scores)}\")\n",
    "        print(f\"Min Score: {np.min(scores)}\")\n",
    "        print(f\"Max Score: {np.max(scores)}\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\n",
    "    scores = []\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\n",
    "    stream_list = []\n",
    "    for df in data_list:\n",
    "        stream_list.append(get_streams(df))\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\n",
    "    for _ in num_range:\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\n",
    "            X_test, y_test = X[p1], y[p1]\n",
    "            X_train = X[p2] + X[p3]\n",
    "            y_train = y[p2] + y[p3]\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            if voting:\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\n",
    "            else:\n",
    "                scores.append(accuracy_score(y_test, pred))\n",
    "    if verbose:\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \n",
    "                            \"avg_score\": [], \"std_score\": [], \"scores\":[]})\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\n",
    "    if progress:\n",
    "        combinations = tqdm(combinations)\n",
    "    for combination in combinations:\n",
    "        model, window_size, overlap_rate = combination\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \"avg_score\": np.mean(scores),\n",
    "                    \"std_score\": np.array(scores).std(),\"scores\": scores}, ignore_index=True)\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\n",
    "    score_df.to_csv(savepath, index=False)\n",
    "    print(f\"result exported to: {savepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speed_acc(x_data):\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\n",
    "    speed = x_data.diff().fillna(0)\n",
    "    acc = speed.diff().fillna(0)\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\n",
    "    return speed, acc\n",
    "\n",
    "def get_speed_acc_jerk(x_data):\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\n",
    "    speed = x_data.diff().fillna(0)\n",
    "    acc = speed.diff().fillna(0)\n",
    "    jerk = acc.diff().fillna(0)\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\n",
    "    jerk.columns = [f\"{col}_jerk\" for col in acc.columns]\n",
    "    return speed, acc, jerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_distance(x_data, joint1, joint2):\n",
    "    \"\"\"\n",
    "    returns the distance between two joints. \n",
    "    \"\"\"\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\n",
    "    return distance\n",
    "\n",
    "def get_all_joint_distances(x_data):\n",
    "    \"\"\"\n",
    "    calculates all the necessary joint distances from the `x_data`, \n",
    "    adds columns to it and returns the modified `x_data`.\n",
    "    the two joints should not be essentially consecutive, \n",
    "    because the distance between two consecutive joints is always constant.\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\n",
    "    \"\"\"\n",
    "    # joints\n",
    "    # Front head        ->  left shoulder       (1->8)\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\n",
    "    # Front head        ->  right shoulder      (1->4)\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\n",
    "    # left shoulder     ->  left wrist          (8->10)\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\n",
    "    # right shoulder    ->  right wrist         (4->7)\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\n",
    "    # v sacral          ->  left elbow          (13->9)\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\n",
    "    # v sacral          ->  right elbow         (13->6)\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\n",
    "    # v sacral          ->  left wrist          (13->10)\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\n",
    "    # v sacral          ->  right wrist         (13->7)\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\n",
    "    # v sacral          ->  rear head           (13->3)\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\n",
    "    # v sacral          ->  top head            (13->2)\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\n",
    "    # left wrist        ->  right wrist         (10->7)\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\n",
    "    # left asis         ->  left wrist          (12->10)\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\n",
    "    # right asis        ->  right wrist         (11->7)\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\n",
    "    # left wrist        ->  top head            (10->2)\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\n",
    "    # right wrist       ->  top head            (7->2)\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\n",
    "    # top head          ->  left asis           (2->12)\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\n",
    "    return x_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_angle(x_data, joint1, joint2, joint3):\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\n",
    "    x3, y3, z3 = x_data[f\"{joint3}_X\"], x_data[f\"{joint3}_Y\"], x_data[f\"{joint3}_Z\"]\n",
    "    v1 = np.array([x2-x1, y2-y1, z2-z1]).T\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2]).T\n",
    "    v1_unit = v1/np.expand_dims(np.linalg.norm(v1, axis=1), axis=1)\n",
    "    v2_unit = v2/np.expand_dims(np.linalg.norm(v2, axis=1), axis=1)\n",
    "    angle = np.arccos(np.sum(v1_unit*v2_unit, axis=1)) # dot multiplication\n",
    "    return angle\n",
    "\n",
    "def get_all_joint_angles(x_data):\n",
    "    # joints\n",
    "    # left shoulder     ->  left elbow      ->  left wrist      (8->9->10)\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\n",
    "    # right shoulder    ->  right elbow     ->  right wrist     (4->6->7)\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\n",
    "    # right shoulder    ->  left shoulder   ->  front head      (4->8->1)\n",
    "    x_data[\"angle_RS_LS_FH\"] = joint_angle(x_data, \"RS\", \"LS\", \"FH\")\n",
    "    # right shoulder    ->  left shoulder   ->  left elbow      (4->8->9)\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\n",
    "    # left shoulder     ->  right shoulder  ->  right elbow     (8->4->6)\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\n",
    "    # v sacral          ->  right offset    ->  rear head       (13->5->3)\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\n",
    "    # vsacral           ->  top head        ->  front head      (13->2->1)\n",
    "    x_data[\"angle_VS_TH_FH\"] = joint_angle(x_data, \"VS\", \"TH\", \"FH\")\n",
    "    # v sacral          ->  left shoulder   ->  left elbow      (13->8->9)\n",
    "    x_data[\"angle_VS_LS_LE\"] = joint_angle(x_data, \"VS\", \"LS\", \"LE\")\n",
    "    # v sacral          ->  right shoulder  ->  right elbow     (13->4->6)\n",
    "    x_data[\"angle_VS_RS_RE\"] = joint_angle(x_data, \"VS\", \"RS\", \"RE\")\n",
    "    # left asis         ->  left shoulder   ->  left elbow      (12->8->9)\n",
    "    x_data[\"angle_LA_LS_LE\"] = joint_angle(x_data, \"LA\", \"LS\", \"LE\")\n",
    "    # right asis        -> right shoulder   ->  right elbow     (11->4->6)\n",
    "    x_data[\"angle_RA_RS_RE\"] = joint_angle(x_data, \"RA\", \"RS\", \"RE\")\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plane_angles(x_data, joint1, joint2):\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\n",
    "    v = np.array([x2-x1, y2-y1, z2-z1]).T\n",
    "    vx = np.array([1, 0, 0])\n",
    "    vy = np.array([0, 1, 0])\n",
    "    vz = np.array([0, 0, 1])\n",
    "    v_unit = v/np.expand_dims(np.linalg.norm(v, axis=1), axis=1)\n",
    "    angle_x = np.arccos(np.sum(v_unit*vx, axis=1)) \n",
    "    angle_y = np.arccos(np.sum(v_unit*vy, axis=1)) \n",
    "    angle_z = np.arccos(np.sum(v_unit*vz, axis=1)) \n",
    "    return angle_x, angle_y, angle_z\n",
    "\n",
    "def get_all_angles_with_plane(x_data):\n",
    "    # joints\n",
    "    # arm angles: left shoulder     ->  left elbow \n",
    "    x_data[\"angle_x_LS_LE\"], x_data[\"angle_y_LS_LE\"], x_data[\"angle_z_LS_LE\"] = plane_angles(x_data, \"LS\", \"LE\")\n",
    "    # forearm angles: left elbow    ->  left  wrist\n",
    "    x_data[\"angle_x_LE_LW\"], x_data[\"angle_y_LE_LW\"], x_data[\"angle_z_LE_LW\"] = plane_angles(x_data, \"LE\", \"LW\")\n",
    "    # arm angles: right shoulder    ->  right elbow \n",
    "    x_data[\"angle_x_RS_RE\"], x_data[\"angle_y_RS_RE\"], x_data[\"angle_z_RS_RE\"] = plane_angles(x_data, \"RS\", \"RE\")\n",
    "    # forearm angles: right elbow   ->  right  wrist\n",
    "    x_data[\"angle_x_RE_RW\"], x_data[\"angle_y_RE_RW\"], x_data[\"angle_z_RE_RW\"] = plane_angles(x_data, \"RE\", \"RW\")\n",
    "    # backbone angles: v sacral     ->  rear head\n",
    "    x_data[\"angle_x_VS_RH\"], x_data[\"angle_y_VS_RH\"], x_data[\"angle_z_VS_RH\"] = plane_angles(x_data, \"VS\", \"RH\")\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_best():\n",
    "    def get_streams(x_data):\n",
    "        speed, acc = get_speed_acc(x_data)\n",
    "        x_data = pd.concat([x_data, speed], axis=1)\n",
    "        x_data = get_all_joint_distances(x_data)\n",
    "        return x_data\n",
    "\n",
    "    def get_features(x_data):\n",
    "        features = []\n",
    "        cols = x_data.columns.tolist()\n",
    "        #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \n",
    "        for k in cols:\n",
    "            features.append(x_data[k].std(ddof=0))\n",
    "            # features.append(np.average(x_data[k]))\n",
    "            features.append(np.max(x_data[k]))\n",
    "            features.append(np.min(x_data[k]))\n",
    "            features.append(np.median(x_data[k]))        \n",
    "            # features.append(np.var(x_data[k]))\n",
    "            # fd = np.abs(fft(np.array(x_data[k])))**2\n",
    "            # features.append(stats.skew(fd))\n",
    "            # features.append(stats.kurtosis(fd))\n",
    "        return features\n",
    "    joint_names = ['FH', 'TH', 'RH', 'RS', 'RO', 'RE', 'RW', 'LS', 'LE', 'LW', 'RA', 'LA', 'VS']\n",
    "    pos_cols = [i+j for i in joint_names for j in [\"_X\", \"_Y\", \"_Z\"]]\n",
    "    speed_cols = [f\"{col}_speed\" for col in pos_cols]\n",
    "    acc_cols = [f\"{col}_acc\" for col in pos_cols]\n",
    "    distance_cols = ['dist_FH_LS', 'dist_FH_RS', 'dist_LS_LW', 'dist_RS_RW', 'dist_VS_LE', 'dist_VS_RE', \n",
    "        'dist_VS_LW', 'dist_VS_RW','dist_VS_RH', 'dist_VS_TH', 'dist_LW_RW', 'dist_LA_LW', 'dist_RA_RW', \n",
    "        'dist_LW_TH', 'dist_RW_TH', 'dist_TH_LA']\n",
    "    colnames = pos_cols + speed_cols + acc_cols + distance_cols\n",
    "    return get_streams, get_features, colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 151/151 [00:06<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 512/512 [00:22<00:00, 22.76it/s]\n"
     ]
    }
   ],
   "source": [
    "get_streams, get_features, colnames = current_best()\n",
    "overlap_rate, window_size = 0.8, 4000\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True):\n",
    "    scores = []\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\n",
    "    for _ in num_range:\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\n",
    "            X_test, y_test = X[p1], y[p1]\n",
    "            X_train = X[p2] + X[p3]\n",
    "            y_train = y[p2] + y[p3]\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.fit_transform(X_test)\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            pred = model.predict(X_test_scaled)\n",
    "            if voting:\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\n",
    "            else:\n",
    "                scores.append(accuracy_score(y_test, pred))\n",
    "    if verbose:\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\n",
    "        print(f\"Std Score: {np.std(scores)}\")\n",
    "        print(f\"Min Score: {np.min(scores)}\")\n",
    "        print(f\"Max Score: {np.max(scores)}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:33<00:00,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.7633523843735084\n",
      "Std Score: 0.019890269355005204\n",
      "Min Score: 0.7293233082706767\n",
      "Max Score: 0.8053097345132744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## This cell was used for experimentation\n",
    "## So far best combo: overlap_rate, window_size, n_estimators = 0.8, 4000, 2500; excluding acc\n",
    "\n",
    "model = RFC(2500, criterion=\"gini\", n_jobs=-1) \n",
    "scores = scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:05<00:00, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.715566886015833\n",
      "Std Score: 0.03187665295686838\n",
      "Min Score: 0.6343612334801763\n",
      "Max Score: 0.7961165048543689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RFC(1200, criterion=\"gini\", max_depth=12, n_jobs=-1)\n",
    "scores = model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:21<00:00, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.7183364356708543\n",
      "Std Score: 0.030369686801219008\n",
      "Min Score: 0.6563876651982379\n",
      "Max Score: 0.7572815533980582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RFC(1200, criterion=\"gini\", min_samples_split=6, n_jobs=-1)\n",
    "scores = model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:16<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 736/736 [01:18<00:00,  9.43it/s]\n"
     ]
    }
   ],
   "source": [
    "get_streams, get_features, colnames = current_best()\n",
    "overlap_rate, window_size = 0.75, 3000\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)\n",
    "ft_names = [i+j for i in colnames for j in [\"_std\", \"_max\", \"_min\", \"_med\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:20<00:00, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.7363534084472131\n",
      "Std Score: 0.034126056701583855\n",
      "Min Score: 0.6651982378854625\n",
      "Max Score: 0.8203883495145631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ETC(1800, criterion=\"gini\", min_samples_split=4, max_depth=12, n_jobs=-1)\n",
    "scores = model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:11<00:00, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.7338456671893613\n",
      "Std Score: 0.03239559131793469\n",
      "Min Score: 0.6519823788546255\n",
      "Max Score: 0.8155339805825242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ETC(1800, criterion=\"gini\", max_depth=12, n_jobs=-1)\n",
    "scores = model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = dict(zip(ft_names, model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../imps_asc.txt\", \"w\") as f:\n",
    "    for i in pd.Series(imps).sort_values().index.tolist():\n",
    "        f.write(f\"{i:15}\\t{imps[i]:15}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.copy()\n",
    "for p in [1,2,3]:\n",
    "    x[p] = pd.DataFrame(np.array(x[p]), columns=ft_names)\n",
    "    x[p].drop(columns=to_be_removed, inplace=True)\n",
    "    x[p] = x[p].to_numpy().tolist()\n",
    "to_be_removed = pd.Series(imps).sort_values().index.tolist()[:75]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:03<00:00, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.7265674497392854\n",
      "Std Score: 0.026565626386254284\n",
      "Min Score: 0.6696035242290749\n",
      "Max Score: 0.7766990291262136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ETC(1800, criterion=\"gini\", max_depth=12, n_jobs=-1)\n",
    "scores = model_evaluator(model, x, y, file_lengths, n_repeats=10, voting=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:11<00:00, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.7288466079831541\n",
      "Std Score: 0.030193467747322445\n",
      "Min Score: 0.6475770925110133\n",
      "Max Score: 0.8106796116504854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ETC(1800, criterion=\"gini\", max_depth=12, n_jobs=-1)\n",
    "scores = model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 0, 100\n",
    "plt.figure(figsize=(20,5));\n",
    "plt.bar(list(imps.keys())[start:end], list(imps.values())[start:end]);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_evaluator(model, X, y, file_lengths, ft_names, n_repeats=7, voting=True, verbose=True):\n",
    "    scores = []\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\n",
    "    for _ in num_range:\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\n",
    "            X_test, y_test = X[p1], y[p1]\n",
    "            X_train = X[p2] + X[p3]\n",
    "            y_train = y[p2] + y[p3]\n",
    "            X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "            y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "            # train_data = lgb.Dataset(X_train, label=y_train, feature_name=ft_names)\n",
    "            # test_data = lgb.Dataset(X_test, label=y_test, feature_name=ft_names)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            if voting:\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\n",
    "            else:\n",
    "                scores.append(accuracy_score(y_test, pred))\n",
    "    if verbose:\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\n",
    "        print(f\"Std Score: {np.std(scores)}\")\n",
    "        print(f\"Min Score: {np.min(scores)}\")\n",
    "        print(f\"Max Score: {np.max(scores)}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:16<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 736/736 [01:18<00:00,  9.43it/s]\n"
     ]
    }
   ],
   "source": [
    "get_streams, get_features, colnames = current_best()\n",
    "ft_names = [i+j for i in colnames for j in [\"_std\", \"_max\", \"_min\", \"_med\"]]\n",
    "overlap_rate, window_size = 0.75, 3000\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:20<00:00, 32.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.6370220636964683\n",
      "Std Score: 0.06150786171117007\n",
      "Min Score: 0.5814977973568282\n",
      "Max Score: 0.7227722772277227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(n_estimators=600, n_jobs=-1)\n",
    "scores = lgbm_evaluator(model, X, y, file_lengths, ft_names, n_repeats=10, voting=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_leaves:  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:54<00:00, 23.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.6351950087442771\n",
      "Std Score: 0.04569828538378031\n",
      "Min Score: 0.5991189427312775\n",
      "Max Score: 0.6996699669966997\n",
      "num_leaves:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:52<00:00, 23.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.5871531466905726\n",
      "Std Score: 0.046134842274195266\n",
      "Min Score: 0.5242718446601942\n",
      "Max Score: 0.6336633663366337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for num_leaves in [45, 20]:\n",
    "    print(\"num_leaves: \", num_leaves)\n",
    "    model = lgb.LGBMClassifier(n_estimators=100, n_jobs=-1, objective=\"multiclass\", num_leaves=num_leaves)\n",
    "    scores = lgbm_evaluator(model, X, y, file_lengths, ft_names, n_repeats=10, voting=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 151/151 [00:28<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 736/736 [00:43<00:00, 16.83it/s]\n"
     ]
    }
   ],
   "source": [
    "get_streams, get_features, colnames = current_best()\n",
    "overlap_rate, window_size = 0.75, 3000\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluator(model, X, y, file_lengths, n_repeats=7, voting=True, verbose=True):\n",
    "    scores = []\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\n",
    "    for _ in num_range:\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\n",
    "            X_test, y_test = X[p1], y[p1]\n",
    "            X_train = X[p2] + X[p3]\n",
    "            y_train = y[p2] + y[p3]\n",
    "            X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "            y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            if voting:\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\n",
    "            else:\n",
    "                scores.append(accuracy_score(y_test, pred))\n",
    "    if verbose:\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\n",
    "        print(f\"Std Score: {np.std(scores)}\")\n",
    "        print(f\"Min Score: {np.min(scores)}\")\n",
    "        print(f\"Max Score: {np.max(scores)}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True):\n",
    "    scores = []\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\n",
    "    for _ in num_range:\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\n",
    "            X_test, y_test = X[p1], y[p1]\n",
    "            X_train = X[p2] + X[p3]\n",
    "            y_train = y[p2] + y[p3]\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.fit_transform(X_test)\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            pred = model.predict(X_test_scaled)\n",
    "            if voting:\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\n",
    "            else:\n",
    "                scores.append(accuracy_score(y_test, pred))\n",
    "    if verbose:\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\n",
    "        print(f\"Std Score: {np.std(scores)}\")\n",
    "        print(f\"Min Score: {np.min(scores)}\")\n",
    "        print(f\"Max Score: {np.max(scores)}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 10%|████████▎                                                                          | 1/10 [00:15<02:18, 15.38s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:30<02:03, 15.42s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:47<01:50, 15.84s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [01:03<01:35, 15.96s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [01:19<01:20, 16.05s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [01:35<01:04, 16.06s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [01:51<00:47, 15.86s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [02:06<00:31, 15.76s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [02:21<00:15, 15.62s/it]c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\aponaloy shahin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:37<00:00, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.4818193893295492\n",
      "Std Score: 0.0043133688089684085\n",
      "Min Score: 0.47572815533980584\n",
      "Max Score: 0.48514851485148514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(colsample_bytree = 1, max_depth = 5, subsample = 1, n_estimators=100, learning_rate = 0.01, objective = \"multi:softmax\", eval_metric = \"merror\")\n",
    "scores = scaled_model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
