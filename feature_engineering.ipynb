{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "from tqdm.notebook import tqdm, trange\r\n",
    "from scipy import signal, stats\r\n",
    "from scipy.fft import fft\r\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\r\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Internal functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    \r\n",
    "    # make a list for segment window and its label\r\n",
    "    seg_data = []\r\n",
    "\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    \r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    #segment\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "        \r\n",
    "    return seg_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature related functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "\r\n",
    "def get_speed_acc_jerk(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    jerk = acc.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    jerk.columns = [f\"{col}_jerk\" for col in acc.columns]\r\n",
    "    return speed, acc, jerk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def joint_angle(x_data, joint1, joint2, joint3):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    x3, y3, z3 = x_data[f\"{joint3}_X\"], x_data[f\"{joint3}_Y\"], x_data[f\"{joint3}_Z\"]\r\n",
    "    v1 = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2]).T\r\n",
    "    v1_unit = v1/np.expand_dims(np.linalg.norm(v1, axis=1), axis=1)\r\n",
    "    v2_unit = v2/np.expand_dims(np.linalg.norm(v2, axis=1), axis=1)\r\n",
    "    angle = np.arccos(np.sum(v1_unit*v2_unit, axis=1)) # dot multiplication\r\n",
    "    return angle\r\n",
    "\r\n",
    "def get_all_joint_angles(x_data):\r\n",
    "    # joints\r\n",
    "    # left shoulder     ->  left elbow      ->  left wrist      (8->9->10)\r\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\r\n",
    "    # right shoulder    ->  right elbow     ->  right wrist     (4->6->7)\r\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  front head      (4->8->1)\r\n",
    "    x_data[\"angle_RS_LS_FH\"] = joint_angle(x_data, \"RS\", \"LS\", \"FH\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  left elbow      (4->8->9)\r\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\r\n",
    "    # left shoulder     ->  right shoulder  ->  right elbow     (8->4->6)\r\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\r\n",
    "    # v sacral          ->  right offset    ->  rear head       (13->5->3)\r\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\r\n",
    "    # vsacral           ->  top head        ->  front head      (13->2->1)\r\n",
    "    x_data[\"angle_VS_TH_FH\"] = joint_angle(x_data, \"VS\", \"TH\", \"FH\")\r\n",
    "    # v sacral          ->  left shoulder   ->  left elbow      (13->8->9)\r\n",
    "    x_data[\"angle_VS_LS_LE\"] = joint_angle(x_data, \"VS\", \"LS\", \"LE\")\r\n",
    "    # v sacral          ->  right shoulder  ->  right elbow     (13->4->6)\r\n",
    "    x_data[\"angle_VS_RS_RE\"] = joint_angle(x_data, \"VS\", \"RS\", \"RE\")\r\n",
    "    # left asis         ->  left shoulder   ->  left elbow      (12->8->9)\r\n",
    "    x_data[\"angle_LA_LS_LE\"] = joint_angle(x_data, \"LA\", \"LS\", \"LE\")\r\n",
    "    # right asis        -> right shoulder   ->  right elbow     (11->4->6)\r\n",
    "    x_data[\"angle_RA_RS_RE\"] = joint_angle(x_data, \"RA\", \"RS\", \"RE\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def plane_angles(x_data, joint1, joint2):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    v = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    vx = np.array([1, 0, 0])\r\n",
    "    vy = np.array([0, 1, 0])\r\n",
    "    vz = np.array([0, 0, 1])\r\n",
    "    v_unit = v/np.expand_dims(np.linalg.norm(v, axis=1), axis=1)\r\n",
    "    angle_x = np.arccos(np.sum(v_unit*vx, axis=1)) \r\n",
    "    angle_y = np.arccos(np.sum(v_unit*vy, axis=1)) \r\n",
    "    angle_z = np.arccos(np.sum(v_unit*vz, axis=1)) \r\n",
    "    return angle_x, angle_y, angle_z\r\n",
    "\r\n",
    "def get_all_angles_with_plane(x_data):\r\n",
    "    # joints\r\n",
    "    # arm angles: left shoulder     ->  left elbow \r\n",
    "    x_data[\"angle_x_LS_LE\"], x_data[\"angle_y_LS_LE\"], x_data[\"angle_z_LS_LE\"] = plane_angles(x_data, \"LS\", \"LE\")\r\n",
    "    # forearm angles: left elbow    ->  left  wrist\r\n",
    "    x_data[\"angle_x_LE_LW\"], x_data[\"angle_y_LE_LW\"], x_data[\"angle_z_LE_LW\"] = plane_angles(x_data, \"LE\", \"LW\")\r\n",
    "    # arm angles: right shoulder    ->  right elbow \r\n",
    "    x_data[\"angle_x_RS_RE\"], x_data[\"angle_y_RS_RE\"], x_data[\"angle_z_RS_RE\"] = plane_angles(x_data, \"RS\", \"RE\")\r\n",
    "    # forearm angles: right elbow   ->  right  wrist\r\n",
    "    x_data[\"angle_x_RE_RW\"], x_data[\"angle_y_RE_RW\"], x_data[\"angle_z_RE_RW\"] = plane_angles(x_data, \"RE\", \"RW\")\r\n",
    "    # backbone angles: v sacral     ->  rear head\r\n",
    "    x_data[\"angle_x_VS_RH\"], x_data[\"angle_y_VS_RH\"], x_data[\"angle_z_VS_RH\"] = plane_angles(x_data, \"VS\", \"RH\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def num_zero_crossings(series):\r\n",
    "    arr = series.to_numpy()\r\n",
    "    return ((arr[:-1] * arr[1:]) < 0).sum()\r\n",
    "\r\n",
    "def get_peaks(series):\r\n",
    "    peaks, _ = signal.find_peaks(series)\r\n",
    "    num_peaks = len(peaks)\r\n",
    "    if not num_peaks:\r\n",
    "        return 0, 0, 0, 0\r\n",
    "    mean_diff = np.diff(peaks).mean() if num_peaks>1 else 0\r\n",
    "    mean_height = series[peaks].mean()\r\n",
    "    std_height = series[peaks].std()\r\n",
    "    return num_peaks, mean_diff, mean_height, std_height\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Operation functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "def get_processed_dataset(overlap_rate, window_size, verbose=True):\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    return X, y, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def model_evaluator(model, X, y, file_lengths, n_repeats=7, voting=True, verbose=True):\r\n",
    "    scores = []\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"Mean Score: {np.mean(scores)}\")\r\n",
    "        print(f\"Std Score: {np.std(scores)}\")\r\n",
    "        print(f\"Min Score: {np.min(scores)}\")\r\n",
    "        print(f\"Max Score: {np.max(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions to be tuned"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        # time domain features\r\n",
    "        features.append(x_data[k].std(ddof=0))          # std\r\n",
    "        features.append(np.average(x_data[k]))          # average\r\n",
    "        features.append(np.max(x_data[k]))              # max\r\n",
    "        features.append(np.min(x_data[k]))              # min\r\n",
    "        features.append(np.median(x_data[k]))           # median\r\n",
    "        features.append(np.var(x_data[k]))              # variance\r\n",
    "        # extra time features\r\n",
    "        features.append(num_zero_crossings(x_data[k]))  # num of zero crossing\r\n",
    "        features.extend(get_peaks(x_data[k]))           # peak related features\r\n",
    "        features.append((x_data[k]**2).sum())           # energy\r\n",
    "        features.append(stats.iqr(x_data[k]))           # inter quartile range\r\n",
    "\r\n",
    "        # freq domain features\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2        # freq domain signal\r\n",
    "        features.append(stats.skew(fd))                 # skew\r\n",
    "        features.append(stats.kurtosis(fd))             # kurtosis\r\n",
    "        # extra freq domain features\r\n",
    "        features.append(fd.std(ddof=0))                 # std\r\n",
    "        features.append(np.average(fd))                 # average\r\n",
    "        features.append(np.max(fd))                     # max\r\n",
    "        features.append(np.min(fd))                     # min\r\n",
    "        features.append(np.median(fd))                  # median\r\n",
    "        features.append(stats.iqr(fd))           # inter quartile range\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_angles(x_data)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    x_data = get_all_angles_with_plane(x_data)\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# driver code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Need to change get_streams, get_features\r\n",
    "\r\n",
    "# load the data into features\r\n",
    "overlap_rate, window_size = 0.75, 3000\r\n",
    "X, y, file_lengths = get_processed_dataset(overlap_rate, window_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = ETC(1800, criterion=\"gini\", min_samples_split=4, max_depth=12, n_jobs=-1)\r\n",
    "scores = model_evaluator(model, X, y, file_lengths, n_repeats=10, voting=True, verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# data_list = []\r\n",
    "# for file in tqdm(glob.glob(\"../TrainData/*/*/*.csv\")):\r\n",
    "#     tempdf = pd.read_csv(file)\r\n",
    "#     tempdf = rename_columns(tempdf)\r\n",
    "#     data_list.extend(segmentation(tempdf, 0.5, 500))\r\n",
    "\r\n",
    "# stream_list = []\r\n",
    "# for df in tqdm(data_list):\r\n",
    "#     stream_list.append(get_streams(df))\r\n",
    "\r\n",
    "\r\n",
    "# features_list = []\r\n",
    "# label_list = []\r\n",
    "# for j in trange(0,len(stream_list)):\r\n",
    "#     #extract only xyz columns\r\n",
    "#     x_data = stream_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "\r\n",
    "#     #Get features and label for each elements\r\n",
    "#     features_list.append(get_features(x_data))\r\n",
    "#     label_list.append(stream_list[j].iloc[0, stream_list[j].columns.tolist().index(\"activity\")])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/3573 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "600b554a71694f5ab60c17a7da8119d0"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "rf = RFC(300, n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\r\n",
    "n_scores = cross_val_score(rf, features_list, label_list, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise', verbose=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.1min finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "n_scores.mean(), n_scores"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.8430089354177424,\n",
       " array([0.79050279, 0.84078212, 0.82681564, 0.85994398, 0.86554622,\n",
       "        0.86834734, 0.82352941, 0.85434174, 0.85434174, 0.84593838]))"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}