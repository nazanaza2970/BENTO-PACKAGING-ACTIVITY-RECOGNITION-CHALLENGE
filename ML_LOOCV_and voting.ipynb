{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import glob\r\n",
    "from datetime import datetime\r\n",
    "import os\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from scipy import stats, signal\r\n",
    "from scipy.stats import mode\r\n",
    "from scipy.fft import fft\r\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, ExtraTreesClassifier as ETC\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, StratifiedKFold\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "from tqdm import tqdm, trange"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "def std_score(file):\r\n",
    "    df = pd.read_csv(file)\r\n",
    "    df.sort_values(\"avg_score\", ascending=False, inplace=True)\r\n",
    "    df[\"scores\"] = df[\"scores\"].apply(json.loads)\r\n",
    "    df[\"std_score\"] = df[\"scores\"].apply(lambda x: np.array(x).std())\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "list(map(lambda x: x[:-2], stream_list[0].columns.tolist()[:39:3]))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['FH', 'TH', 'RH', 'RS', 'RO', 'RE', 'RW', 'LS', 'LE', 'LW', 'RA', 'LA', 'VS']"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "\r\n",
    "df = std_score(\"../12. again_with_freq_speed_acc_angles_distances_2021-08-16 15.42.02.csv\")\r\n",
    "df[[\"model\", \"window_size\", \"overlap_rate\", \"avg_score\", \"std_score\"]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                model  window_size  \\\n",
       "15  ExtraTreesClassifier(n_estimators=1200, n_jobs...       4000.0   \n",
       "14  ExtraTreesClassifier(n_estimators=1200, n_jobs...       4000.0   \n",
       "10  ExtraTreesClassifier(n_estimators=600, n_jobs=-1)       4000.0   \n",
       "11  ExtraTreesClassifier(n_estimators=600, n_jobs=-1)       4000.0   \n",
       "6   RandomForestClassifier(n_estimators=1200, n_jo...       4000.0   \n",
       "12  ExtraTreesClassifier(n_estimators=1200, n_jobs...       3000.0   \n",
       "8   ExtraTreesClassifier(n_estimators=600, n_jobs=-1)       3000.0   \n",
       "4   RandomForestClassifier(n_estimators=1200, n_jo...       3000.0   \n",
       "3   RandomForestClassifier(n_estimators=600, n_job...       4000.0   \n",
       "0   RandomForestClassifier(n_estimators=600, n_job...       3000.0   \n",
       "7   RandomForestClassifier(n_estimators=1200, n_jo...       4000.0   \n",
       "2   RandomForestClassifier(n_estimators=600, n_job...       4000.0   \n",
       "13  ExtraTreesClassifier(n_estimators=1200, n_jobs...       3000.0   \n",
       "9   ExtraTreesClassifier(n_estimators=600, n_jobs=-1)       3000.0   \n",
       "1   RandomForestClassifier(n_estimators=600, n_job...       3000.0   \n",
       "5   RandomForestClassifier(n_estimators=1200, n_jo...       3000.0   \n",
       "\n",
       "    overlap_rate  avg_score  std_score  \n",
       "15          0.75   0.735882   0.091278  \n",
       "14          0.50   0.729047   0.105386  \n",
       "10          0.50   0.722271   0.098716  \n",
       "11          0.75   0.711079   0.087443  \n",
       "6           0.50   0.702202   0.103027  \n",
       "12          0.50   0.698316   0.079047  \n",
       "8           0.50   0.693013   0.078084  \n",
       "4           0.50   0.682441   0.075962  \n",
       "3           0.75   0.672543   0.069759  \n",
       "0           0.50   0.672370   0.085229  \n",
       "7           0.75   0.671598   0.061811  \n",
       "2           0.50   0.668396   0.067832  \n",
       "13          0.75   0.658676   0.089811  \n",
       "9           0.75   0.650500   0.090750  \n",
       "1           0.75   0.634747   0.085237  \n",
       "5           0.75   0.629366   0.102156  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>window_size</th>\n",
       "      <th>overlap_rate</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>std_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=1200, n_jobs...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.735882</td>\n",
       "      <td>0.091278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=1200, n_jobs...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.729047</td>\n",
       "      <td>0.105386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=600, n_jobs=-1)</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.722271</td>\n",
       "      <td>0.098716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=600, n_jobs=-1)</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.711079</td>\n",
       "      <td>0.087443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier(n_estimators=1200, n_jo...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.702202</td>\n",
       "      <td>0.103027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=1200, n_jobs...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.698316</td>\n",
       "      <td>0.079047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=600, n_jobs=-1)</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.693013</td>\n",
       "      <td>0.078084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier(n_estimators=1200, n_jo...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.682441</td>\n",
       "      <td>0.075962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier(n_estimators=600, n_job...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.672543</td>\n",
       "      <td>0.069759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier(n_estimators=600, n_job...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.672370</td>\n",
       "      <td>0.085229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier(n_estimators=1200, n_jo...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.671598</td>\n",
       "      <td>0.061811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier(n_estimators=600, n_job...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.668396</td>\n",
       "      <td>0.067832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=1200, n_jobs...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.658676</td>\n",
       "      <td>0.089811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesClassifier(n_estimators=600, n_jobs=-1)</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.650500</td>\n",
       "      <td>0.090750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier(n_estimators=600, n_job...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.634747</td>\n",
       "      <td>0.085237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier(n_estimators=1200, n_jo...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.629366</td>\n",
       "      <td>0.102156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline Leave One Out"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].iloc[0, -2]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    X, y = feature_extractor(data_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gridsearch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, overlap_rate, window_size = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [2000, 3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "model = RFC(300, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.5, 1000, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:10<00:00, 14.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1667/1667 [00:55<00:00, 29.84it/s]\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.01s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.3548648832961903\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "model = ETC(1500, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, voting=False, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:10<00:00, 14.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:15<00:00, 27.12it/s]\n",
      "100%|██████████| 5/5 [00:40<00:00,  8.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5757194532188168\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency domain features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "\r\n",
    "        features.append(fd.std(ddof=0))\r\n",
    "        features.append(np.average(fd))\r\n",
    "        features.append(np.max(fd))\r\n",
    "        features.append(np.min(fd))\r\n",
    "        features.append(np.median(fd))                                \r\n",
    "        features.append(np.var(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].iloc[0, -2]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].iloc[0, -1])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    X, y = feature_extractor(data_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", n_repeats=7, verbose=False):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models = parameters[\"model\"]\r\n",
    "    for model in models:\r\n",
    "        for window_size in parameters[\"window_size\"]:\r\n",
    "            for overlap_rate in parameters[\"overlap_rate\"]:\r\n",
    "                scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "                score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                            \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                            \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/grid_search_result_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [2000, 3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = RFC(600, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, n_repeats=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:09<00:00, 16.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:24<00:00, 16.66it/s]\n",
      "100%|██████████| 10/10 [00:42<00:00,  4.25s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5870291343610226\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = RFC(1500, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:09<00:00, 15.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:25<00:00, 16.42it/s]\n",
      "100%|██████████| 5/5 [00:57<00:00, 11.41s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5743854041577472\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with speed, acceleration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "    \r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        # fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        # features.append(stats.skew(fd))\r\n",
    "        # features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [2000, 3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "model = ETC(1200, n_jobs=-1)\r\n",
    "data_list, file_lengths = dataloader(0.5, 4000, verbose=True)\r\n",
    "stream_list = []\r\n",
    "for df in data_list:\r\n",
    "    stream_list.append(get_streams(df))\r\n",
    "X, y = feature_extractor(stream_list, verbose=True)\r\n",
    "p1, p2, p3 = 1,2,3\r\n",
    "X_test, y_test = X[p1], y[p1]\r\n",
    "X_train = X[p2] + X[p3]\r\n",
    "y_train = y[p2] + y[p3]\r\n",
    "# print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "model.fit(X_train, y_train)\r\n",
    "pred = model.predict(X_test)\r\n",
    "accuracy_score(pred, y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:10<00:00, 13.99it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 240/240 [01:03<00:00,  3.79it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6962025316455697"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "cols = stream_list[0].columns.tolist()\r\n",
    "cols.remove(\"activity\")\r\n",
    "cols.remove(\"subject_id\")\r\n",
    "print(len(cols))\r\n",
    "colnames = []\r\n",
    "for c in tqdm(cols):\r\n",
    "    for suffix in [\"_std\", \"avg\", \"max\", \"min\", \"med\", \"var\", \"skew\", \"kurt\"]:\r\n",
    "        colnames.append(c+suffix)\r\n",
    "print(len(colnames))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "117\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 117/117 [00:00<00:00, 10663.25it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "936\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration, joint distances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"again_with_freq_speed_acc_distance\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [50:48<00:00, 190.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../again_with_freq_speed_acc_distance_2021-08-16 14.15.59.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with less_feature, speed, acceleration, distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        # features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        # features.append(np.var(x_data[k]))\r\n",
    "        # fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        # features.append(stats.skew(fd))\r\n",
    "        # features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"again_with_less_feat_speed_acc_distance\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [35:55<00:00, 134.70s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../again_with_freq_speed_acc_distance_2021-08-16 16.37.15.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with speed, acceleration, distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "ddef get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        # fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        # features.append(stats.skew(fd))\r\n",
    "        # features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration and angles "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def joint_angle(x_data, joint1, joint2, joint3):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    x3, y3, z3 = x_data[f\"{joint3}_X\"], x_data[f\"{joint3}_Y\"], x_data[f\"{joint3}_Z\"]\r\n",
    "    v1 = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2]).T\r\n",
    "    v1_unit = v1/np.expand_dims(np.linalg.norm(v1, axis=1), axis=1)\r\n",
    "    v2_unit = v2/np.expand_dims(np.linalg.norm(v2, axis=1), axis=1)\r\n",
    "    angle = np.arccos(np.sum(v1_unit*v2_unit, axis=1)) # dot multiplication\r\n",
    "    return angle\r\n",
    "\r\n",
    "def get_all_joint_angles(x_data):\r\n",
    "    # joints\r\n",
    "    # left shoulder     ->  left elbow      ->  left wrist      (8->9->10)\r\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\r\n",
    "    # right shoulder    ->  right elbow     ->  right wrist     (4->6->7)\r\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  front head      (4->8->1)\r\n",
    "    x_data[\"angle_RS_LS_FH\"] = joint_angle(x_data, \"RS\", \"LS\", \"FH\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  left elbow      (4->8->9)\r\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\r\n",
    "    # left shoulder     ->  right shoulder  ->  right elbow     (8->4->6)\r\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\r\n",
    "    # v sacral          ->  right offset    ->  rear head       (13->5->3)\r\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\r\n",
    "    # vsacral           ->  top head        ->  front head      (13->2->1)\r\n",
    "    x_data[\"angle_VS_TH_FH\"] = joint_angle(x_data, \"VS\", \"TH\", \"FH\")\r\n",
    "    # v sacral          ->  left shoulder   ->  left elbow      (13->8->9)\r\n",
    "    x_data[\"angle_VS_LS_LE\"] = joint_angle(x_data, \"VS\", \"LS\", \"LE\")\r\n",
    "    # v sacral          ->  right shoulder  ->  right elbow     (13->4->6)\r\n",
    "    x_data[\"angle_VS_RS_RE\"] = joint_angle(x_data, \"VS\", \"RS\", \"RE\")\r\n",
    "    # left asis         ->  left shoulder   ->  left elbow      (12->8->9)\r\n",
    "    x_data[\"angle_LA_LS_LE\"] = joint_angle(x_data, \"LA\", \"LS\", \"LE\")\r\n",
    "    # right asis        -> right shoulder   ->  right elbow     (11->4->6)\r\n",
    "    x_data[\"angle_RA_RS_RE\"] = joint_angle(x_data, \"RA\", \"RS\", \"RE\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_angles(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "              ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"again_with_freq_speed_acc_angles\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [1:17:55<00:00, 233.78s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../gridCV_results_2021-08-14 19.01.56.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration, distance, angle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def joint_angle(x_data, joint1, joint2, joint3):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    x3, y3, z3 = x_data[f\"{joint3}_X\"], x_data[f\"{joint3}_Y\"], x_data[f\"{joint3}_Z\"]\r\n",
    "    v1 = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2]).T\r\n",
    "    v1_unit = v1/np.expand_dims(np.linalg.norm(v1, axis=1), axis=1)\r\n",
    "    v2_unit = v2/np.expand_dims(np.linalg.norm(v2, axis=1), axis=1)\r\n",
    "    angle = np.arccos(np.sum(v1_unit*v2_unit, axis=1)) # dot multiplication\r\n",
    "    return angle\r\n",
    "\r\n",
    "def get_all_joint_angles(x_data):\r\n",
    "    # joints\r\n",
    "    # left shoulder     ->  left elbow      ->  left wrist      (8->9->10)\r\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\r\n",
    "    # right shoulder    ->  right elbow     ->  right wrist     (4->6->7)\r\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  front head      (4->8->1)\r\n",
    "    x_data[\"angle_RS_LS_FH\"] = joint_angle(x_data, \"RS\", \"LS\", \"FH\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  left elbow      (4->8->9)\r\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\r\n",
    "    # left shoulder     ->  right shoulder  ->  right elbow     (8->4->6)\r\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\r\n",
    "    # v sacral          ->  right offset    ->  rear head       (13->5->3)\r\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\r\n",
    "    # vsacral           ->  top head        ->  front head      (13->2->1)\r\n",
    "    x_data[\"angle_VS_TH_FH\"] = joint_angle(x_data, \"VS\", \"TH\", \"FH\")\r\n",
    "    # v sacral          ->  left shoulder   ->  left elbow      (13->8->9)\r\n",
    "    x_data[\"angle_VS_LS_LE\"] = joint_angle(x_data, \"VS\", \"LS\", \"LE\")\r\n",
    "    # v sacral          ->  right shoulder  ->  right elbow     (13->4->6)\r\n",
    "    x_data[\"angle_VS_RS_RE\"] = joint_angle(x_data, \"VS\", \"RS\", \"RE\")\r\n",
    "    # left asis         ->  left shoulder   ->  left elbow      (12->8->9)\r\n",
    "    x_data[\"angle_LA_LS_LE\"] = joint_angle(x_data, \"LA\", \"LS\", \"LE\")\r\n",
    "    # right asis        -> right shoulder   ->  right elbow     (11->4->6)\r\n",
    "    x_data[\"angle_RA_RS_RE\"] = joint_angle(x_data, \"RA\", \"RS\", \"RE\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    x_data = get_all_joint_angles(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"again_with_speed_acc_angles_distances\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [1:16:08<00:00, 285.52s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../again_with_speed_acc_angles_distances_2021-08-16 15.42.02.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration, distance, plane"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with speed, acceleration, distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "ddef get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        # fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        # features.append(stats.skew(fd))\r\n",
    "        # features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration and angles "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def joint_angle(x_data, joint1, joint2, joint3):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    x3, y3, z3 = x_data[f\"{joint3}_X\"], x_data[f\"{joint3}_Y\"], x_data[f\"{joint3}_Z\"]\r\n",
    "    v1 = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2]).T\r\n",
    "    v1_unit = v1/np.expand_dims(np.linalg.norm(v1, axis=1), axis=1)\r\n",
    "    v2_unit = v2/np.expand_dims(np.linalg.norm(v2, axis=1), axis=1)\r\n",
    "    angle = np.arccos(np.sum(v1_unit*v2_unit, axis=1)) # dot multiplication\r\n",
    "    return angle\r\n",
    "\r\n",
    "def get_all_joint_angles(x_data):\r\n",
    "    # joints\r\n",
    "    # left shoulder     ->  left elbow      ->  left wrist      (8->9->10)\r\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\r\n",
    "    # right shoulder    ->  right elbow     ->  right wrist     (4->6->7)\r\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  front head      (4->8->1)\r\n",
    "    x_data[\"angle_RS_LS_FH\"] = joint_angle(x_data, \"RS\", \"LS\", \"FH\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  left elbow      (4->8->9)\r\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\r\n",
    "    # left shoulder     ->  right shoulder  ->  right elbow     (8->4->6)\r\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\r\n",
    "    # v sacral          ->  right offset    ->  rear head       (13->5->3)\r\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\r\n",
    "    # vsacral           ->  top head        ->  front head      (13->2->1)\r\n",
    "    x_data[\"angle_VS_TH_FH\"] = joint_angle(x_data, \"VS\", \"TH\", \"FH\")\r\n",
    "    # v sacral          ->  left shoulder   ->  left elbow      (13->8->9)\r\n",
    "    x_data[\"angle_VS_LS_LE\"] = joint_angle(x_data, \"VS\", \"LS\", \"LE\")\r\n",
    "    # v sacral          ->  right shoulder  ->  right elbow     (13->4->6)\r\n",
    "    x_data[\"angle_VS_RS_RE\"] = joint_angle(x_data, \"VS\", \"RS\", \"RE\")\r\n",
    "    # left asis         ->  left shoulder   ->  left elbow      (12->8->9)\r\n",
    "    x_data[\"angle_LA_LS_LE\"] = joint_angle(x_data, \"LA\", \"LS\", \"LE\")\r\n",
    "    # right asis        -> right shoulder   ->  right elbow     (11->4->6)\r\n",
    "    x_data[\"angle_RA_RS_RE\"] = joint_angle(x_data, \"RA\", \"RS\", \"RE\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_angles(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "              ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"again_with_freq_speed_acc_angles\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [1:17:55<00:00, 233.78s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../gridCV_results_2021-08-14 19.01.56.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration, distance, angle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def joint_angle(x_data, joint1, joint2, joint3):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    x3, y3, z3 = x_data[f\"{joint3}_X\"], x_data[f\"{joint3}_Y\"], x_data[f\"{joint3}_Z\"]\r\n",
    "    v1 = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2]).T\r\n",
    "    v1_unit = v1/np.expand_dims(np.linalg.norm(v1, axis=1), axis=1)\r\n",
    "    v2_unit = v2/np.expand_dims(np.linalg.norm(v2, axis=1), axis=1)\r\n",
    "    angle = np.arccos(np.sum(v1_unit*v2_unit, axis=1)) # dot multiplication\r\n",
    "    return angle\r\n",
    "\r\n",
    "def get_all_joint_angles(x_data):\r\n",
    "    # joints\r\n",
    "    # left shoulder     ->  left elbow      ->  left wrist      (8->9->10)\r\n",
    "    x_data[\"angle_LS_LE_LW\"] = joint_angle(x_data, \"LS\", \"LE\", \"LW\")\r\n",
    "    # right shoulder    ->  right elbow     ->  right wrist     (4->6->7)\r\n",
    "    x_data[\"angle_RS_RE_RW\"] = joint_angle(x_data, \"RS\", \"RE\", \"RW\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  front head      (4->8->1)\r\n",
    "    x_data[\"angle_RS_LS_FH\"] = joint_angle(x_data, \"RS\", \"LS\", \"FH\")\r\n",
    "    # right shoulder    ->  left shoulder   ->  left elbow      (4->8->9)\r\n",
    "    x_data[\"angle_RS_LS_LE\"] = joint_angle(x_data, \"RS\", \"LS\", \"LE\")\r\n",
    "    # left shoulder     ->  right shoulder  ->  right elbow     (8->4->6)\r\n",
    "    x_data[\"angle_LS_RS_RE\"] = joint_angle(x_data, \"LS\", \"RS\", \"RE\")\r\n",
    "    # v sacral          ->  right offset    ->  rear head       (13->5->3)\r\n",
    "    x_data[\"angle_VS_RO_RH\"] = joint_angle(x_data, \"VS\", \"RO\", \"RH\")\r\n",
    "    # vsacral           ->  top head        ->  front head      (13->2->1)\r\n",
    "    x_data[\"angle_VS_TH_FH\"] = joint_angle(x_data, \"VS\", \"TH\", \"FH\")\r\n",
    "    # v sacral          ->  left shoulder   ->  left elbow      (13->8->9)\r\n",
    "    x_data[\"angle_VS_LS_LE\"] = joint_angle(x_data, \"VS\", \"LS\", \"LE\")\r\n",
    "    # v sacral          ->  right shoulder  ->  right elbow     (13->4->6)\r\n",
    "    x_data[\"angle_VS_RS_RE\"] = joint_angle(x_data, \"VS\", \"RS\", \"RE\")\r\n",
    "    # left asis         ->  left shoulder   ->  left elbow      (12->8->9)\r\n",
    "    x_data[\"angle_LA_LS_LE\"] = joint_angle(x_data, \"LA\", \"LS\", \"LE\")\r\n",
    "    # right asis        -> right shoulder   ->  right elbow     (11->4->6)\r\n",
    "    x_data[\"angle_RA_RS_RE\"] = joint_angle(x_data, \"RA\", \"RS\", \"RE\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    x_data = get_all_joint_angles(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"again_with_speed_acc_angles_distances\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [1:16:08<00:00, 285.52s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../again_with_speed_acc_angles_distances_2021-08-16 15.42.02.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency, speed, acceleration, distance, plane"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def plane_angles(x_data, joint1, joint2):\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    v = np.array([x2-x1, y2-y1, z2-z1]).T\r\n",
    "    vx = np.array([1, 0, 0])\r\n",
    "    vy = np.array([0, 1, 0])\r\n",
    "    vz = np.array([0, 0, 1])\r\n",
    "    v_unit = v/np.expand_dims(np.linalg.norm(v, axis=1), axis=1)\r\n",
    "    angle_x = np.arccos(np.sum(v_unit*vx, axis=1)) \r\n",
    "    angle_y = np.arccos(np.sum(v_unit*vy, axis=1)) \r\n",
    "    angle_z = np.arccos(np.sum(v_unit*vz, axis=1)) \r\n",
    "    return angle_x, angle_y, angle_z\r\n",
    "\r\n",
    "def get_all_angles_with_plane(x_data):\r\n",
    "    # joints\r\n",
    "    # arm angles: left shoulder     ->  left elbow \r\n",
    "    x_data[\"angle_x_LS_LE\"], x_data[\"angle_y_LS_LE\"], x_data[\"angle_z_LS_LE\"] = plane_angles(x_data, \"LS\", \"LE\")\r\n",
    "    # forearm angles: left elbow    ->  left  wrist\r\n",
    "    x_data[\"angle_x_LE_LW\"], x_data[\"angle_y_LE_LW\"], x_data[\"angle_z_LE_LW\"] = plane_angles(x_data, \"LE\", \"LW\")\r\n",
    "    # arm angles: right shoulder    ->  right elbow \r\n",
    "    x_data[\"angle_x_RS_RE\"], x_data[\"angle_y_RS_RE\"], x_data[\"angle_z_RS_RE\"] = plane_angles(x_data, \"RS\", \"RE\")\r\n",
    "    # forearm angles: right elbow   ->  right  wrist\r\n",
    "    x_data[\"angle_x_RE_RW\"], x_data[\"angle_y_RE_RW\"], x_data[\"angle_z_RE_RW\"] = plane_angles(x_data, \"RE\", \"RW\")\r\n",
    "    # backbone angles: v sacral     ->  rear head\r\n",
    "    x_data[\"angle_x_VS_RH\"], x_data[\"angle_y_VS_RH\"], x_data[\"angle_z_VS_RH\"] = plane_angles(x_data, \"VS\", \"RH\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    x_data = get_all_angles_with_plane(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1000, n_jobs=-1), RFC(1500, n_jobs=-1),\r\n",
    "                ETC(1000, n_jobs=-1), ETC(1500, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000, 5000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"with_speed_acc_distances_plane\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [3:00:51<00:00, 361.72s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../with_speed_acc_distances_plane_2021-08-15 20.50.30.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with ex_features, ex_frequency, speed, acceleration, distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def joint_distance(x_data, joint1, joint2):\r\n",
    "    \"\"\"\r\n",
    "    returns the distance between two joints. \r\n",
    "    \"\"\"\r\n",
    "    x1, y1, z1 = x_data[f\"{joint1}_X\"], x_data[f\"{joint1}_Y\"], x_data[f\"{joint1}_Z\"]\r\n",
    "    x2, y2, z2 = x_data[f\"{joint2}_X\"], x_data[f\"{joint2}_Y\"], x_data[f\"{joint2}_Z\"]\r\n",
    "    distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\r\n",
    "    return distance\r\n",
    "\r\n",
    "def get_all_joint_distances(x_data):\r\n",
    "    \"\"\"\r\n",
    "    calculates all the necessary joint distances from the `x_data`, \r\n",
    "    adds columns to it and returns the modified `x_data`.\r\n",
    "    the two joints should not be essentially consecutive, \r\n",
    "    because the distance between two consecutive joints is always constant.\r\n",
    "    For example, distance between left_wrist and left_elbow is always constant.\r\n",
    "    \"\"\"\r\n",
    "    # joints\r\n",
    "    # Front head        ->  left shoulder       (1->8)\r\n",
    "    x_data[\"dist_FH_LS\"] = joint_distance(x_data, \"FH\", \"LS\")\r\n",
    "    # Front head        ->  right shoulder      (1->4)\r\n",
    "    x_data[\"dist_FH_RS\"] = joint_distance(x_data, \"FH\", \"RS\")\r\n",
    "    # left shoulder     ->  left wrist          (8->10)\r\n",
    "    x_data[\"dist_LS_LW\"] = joint_distance(x_data, \"LS\", \"LW\")\r\n",
    "    # right shoulder    ->  right wrist         (4->7)\r\n",
    "    x_data[\"dist_RS_RW\"] = joint_distance(x_data, \"RS\", \"RW\")\r\n",
    "    # v sacral          ->  left elbow          (13->9)\r\n",
    "    x_data[\"dist_VS_LE\"] = joint_distance(x_data, \"VS\", \"LE\")\r\n",
    "    # v sacral          ->  right elbow         (13->6)\r\n",
    "    x_data[\"dist_VS_RE\"] = joint_distance(x_data, \"VS\", \"RE\")\r\n",
    "    # v sacral          ->  left wrist          (13->10)\r\n",
    "    x_data[\"dist_VS_LW\"] = joint_distance(x_data, \"VS\", \"LW\")\r\n",
    "    # v sacral          ->  right wrist         (13->7)\r\n",
    "    x_data[\"dist_VS_RW\"] = joint_distance(x_data, \"VS\", \"RW\")\r\n",
    "    # v sacral          ->  rear head           (13->3)\r\n",
    "    x_data[\"dist_VS_RH\"] = joint_distance(x_data, \"VS\", \"RH\")\r\n",
    "    # v sacral          ->  top head            (13->2)\r\n",
    "    x_data[\"dist_VS_TH\"] = joint_distance(x_data, \"VS\", \"TH\")\r\n",
    "    # left wrist        ->  right wrist         (10->7)\r\n",
    "    x_data[\"dist_LW_RW\"] = joint_distance(x_data, \"LW\", \"RW\")\r\n",
    "    # left asis         ->  left wrist          (12->10)\r\n",
    "    x_data[\"dist_LA_LW\"] = joint_distance(x_data, \"LA\", \"LW\")\r\n",
    "    # right asis        ->  right wrist         (11->7)\r\n",
    "    x_data[\"dist_RA_RW\"] = joint_distance(x_data, \"RA\", \"RW\")\r\n",
    "    # left wrist        ->  top head            (10->2)\r\n",
    "    x_data[\"dist_LW_TH\"] = joint_distance(x_data, \"LW\", \"TH\")\r\n",
    "    # right wrist       ->  top head            (7->2)\r\n",
    "    x_data[\"dist_RW_TH\"] = joint_distance(x_data, \"RW\", \"TH\")\r\n",
    "    # top head          ->  left asis           (2->12)\r\n",
    "    x_data[\"dist_TH_LA\"] = joint_distance(x_data, \"TH\", \"LA\")\r\n",
    "    return x_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def num_zero_crossings(series):\r\n",
    "    arr = series.to_numpy()\r\n",
    "    return ((arr[:-1] * arr[1:]) < 0).sum()\r\n",
    "\r\n",
    "def get_peaks(series):\r\n",
    "    peaks, _ = signal.find_peaks(series)\r\n",
    "    num_peaks = len(peaks)\r\n",
    "    if not num_peaks:\r\n",
    "        return 0, 0, 0, 0\r\n",
    "    mean_diff = np.diff(peaks).mean() if num_peaks>1 else 0\r\n",
    "    mean_height = series[peaks].mean()\r\n",
    "    std_height = series[peaks].std()\r\n",
    "    return num_peaks, mean_diff, mean_height, std_height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    x_data = x_data.drop(columns=[\"activity\", \"subject_id\"])\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    x_data = pd.concat([x_data, speed, acc], axis=1)\r\n",
    "    x_data = get_all_joint_distances(x_data)\r\n",
    "    return x_data\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        # time domain features\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        # extra time features\r\n",
    "        features.append(num_zero_crossings(x_data[k]))  # num of zero crossing\r\n",
    "        features.extend(get_peaks(x_data[k]))           # peak related features\r\n",
    "        features.append((x_data[k]**2).sum())           # energy\r\n",
    "        features.append(stats.iqr(x_data[k]))           # inter quartile range\r\n",
    "\r\n",
    "        # freq domain features\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "        # extra freq domain features\r\n",
    "        features.append(fd.std(ddof=0))\r\n",
    "        features.append(np.average(fd))\r\n",
    "        features.append(np.max(fd))\r\n",
    "        features.append(np.min(fd))\r\n",
    "        features.append(np.median(fd))\r\n",
    "        features.append(stats.iqr(fd))           # inter quartile range\r\n",
    "    return features\r\n",
    "\r\n",
    "def rename_columns(df):\r\n",
    "    df.columns = [\r\n",
    "        \"FH_X\", \"FH_Y\", \"FH_Z\",     #1\r\n",
    "        \"TH_X\", \"TH_Y\", \"TH_Z\",     #2\r\n",
    "        \"RH_X\", \"RH_Y\", \"RH_Z\",     #3\r\n",
    "        \"RS_X\", \"RS_Y\", \"RS_Z\",     #4\r\n",
    "        \"RO_X\", \"RO_Y\", \"RO_Z\",     #5\r\n",
    "        \"RE_X\", \"RE_Y\", \"RE_Z\",     #6\r\n",
    "        \"RW_X\", \"RW_Y\", \"RW_Z\",     #7\r\n",
    "        \"LS_X\", \"LS_Y\", \"LS_Z\",     #8\r\n",
    "        \"LE_X\", \"LE_Y\", \"LE_Z\",     #9\r\n",
    "        \"LW_X\", \"LW_Y\", \"LW_Z\",     #10\r\n",
    "        \"RA_X\", \"RA_Y\", \"RA_Z\",     #11\r\n",
    "        \"LA_X\", \"LA_Y\", \"LA_Z\",     #12\r\n",
    "        \"VS_X\", \"VS_Y\", \"VS_Z\",     #13\r\n",
    "        \"subject_id\", \"activity\",   # Other columns\r\n",
    "    ]\r\n",
    "    return df\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        tempdf = rename_columns(tempdf)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, window_size, overlap_rate = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "            ETC(1000, n_jobs=-1), ETC(1500, n_jobs=-1)],\r\n",
    "    \"window_size\": [3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters, filename=\"with_exfeatures_exfreq_speed_acc_distance\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [1:49:00<00:00, 408.77s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result exported to: ../with_exfeatures_exfreq_speed_acc_distance_2021-08-16 03.11.01.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}