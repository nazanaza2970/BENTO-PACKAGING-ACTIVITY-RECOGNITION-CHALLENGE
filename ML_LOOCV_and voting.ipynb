{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "import glob\r\n",
    "from datetime import datetime\r\n",
    "import os\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from scipy import stats\r\n",
    "from scipy.stats import mode\r\n",
    "from scipy.fft import fft\r\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, ExtraTreesClassifier as ETC\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, StratifiedKFold\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "from tqdm import tqdm, trange"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline Leave One Out"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].iloc[0, -2]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].reset_index(drop=True).loc[0, \"activity\"])\r\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    X, y = feature_extractor(data_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gridsearch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, overlap_rate, window_size = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [2000, 3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "model = RFC(300, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.5, 1000, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:10<00:00, 14.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1667/1667 [00:55<00:00, 29.84it/s]\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.01s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.3548648832961903\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "model = ETC(1500, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, voting=False, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:10<00:00, 14.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:15<00:00, 27.12it/s]\n",
      "100%|██████████| 5/5 [00:40<00:00,  8.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5757194532188168\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "model = ETC(1500, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4500, voting=False, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:10<00:00, 15.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 310/310 [00:09<00:00, 31.55it/s]\n",
      "100%|██████████| 5/5 [00:29<00:00,  5.97s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.565074572417814\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with frequency domain features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "\r\n",
    "        features.append(fd.std(ddof=0))\r\n",
    "        features.append(np.average(fd))\r\n",
    "        features.append(np.max(fd))\r\n",
    "        features.append(np.min(fd))\r\n",
    "        features.append(np.median(fd))                                \r\n",
    "        features.append(np.var(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].iloc[0, -2]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].iloc[0, -1])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    X, y = feature_extractor(data_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", n_repeats=7, verbose=False):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models = parameters[\"model\"]\r\n",
    "    for model in models:\r\n",
    "        for window_size in parameters[\"window_size\"]:\r\n",
    "            for overlap_rate in parameters[\"overlap_rate\"]:\r\n",
    "                scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "                score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                            \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                            \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/grid_search_result_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parameters = {\r\n",
    "    \"model\": [RFC(300, n_jobs=-1), RFC(600, n_jobs=-1), RFC(1200, n_jobs=-1),\r\n",
    "                ETC(300, n_jobs=-1), ETC(600, n_jobs=-1), ETC(1200, n_jobs=-1)],\r\n",
    "    \"window_size\": [2000, 3000, 4000],\r\n",
    "    \"overlap_rate\": [0.5, 0.75]\r\n",
    "}\r\n",
    "GridSearch(parameters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning Experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = RFC(600, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, n_repeats=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:09<00:00, 16.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:24<00:00, 16.66it/s]\n",
      "100%|██████████| 10/10 [00:42<00:00,  4.25s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5870291343610226\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = RFC(1500, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:09<00:00, 15.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:25<00:00, 16.42it/s]\n",
      "100%|██████████| 5/5 [00:57<00:00, 11.41s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5743854041577472\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = ETC(300, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:09<00:00, 16.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:25<00:00, 16.34it/s]\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.56s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5752278254929603\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = ETC(1500, n_jobs=-1)\r\n",
    "scores = LOOCV_train_evaluate(model, 0.75, 4000, n_repeats=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading the data...\t"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [00:09<00:00, 15.59it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting the features...  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 414/414 [00:25<00:00, 16.50it/s]\n",
      "100%|██████████| 5/5 [00:38<00:00,  7.66s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Mean Score: 0.5693867039976331\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOOCV with speed, acc"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def segmentation(df, overlap_rate, time_window):\r\n",
    "    seg_data = []\r\n",
    "    #convert overlap rate to step for sliding window\r\n",
    "    overlap = int((1 - overlap_rate)*time_window)\r\n",
    "    # interpolate\r\n",
    "    df = df.interpolate().ffill().fillna(0)\r\n",
    "    for i in range(0, len(df)-time_window+1, overlap):\r\n",
    "        seg_data.append(df.loc[i:i+time_window-1, :].copy().reset_index(drop=True))\r\n",
    "    return seg_data\r\n",
    "\r\n",
    "def get_speed_acc(x_data):\r\n",
    "    speed = x_data.diff().fillna(0)\r\n",
    "    acc = speed.diff().fillna(0)\r\n",
    "    speed.columns = [f\"{col}_speed\" for col in speed.columns]\r\n",
    "    acc.columns = [f\"{col}_acc\" for col in acc.columns]\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_streams(x_data):\r\n",
    "    speed, acc = get_speed_acc(x_data)\r\n",
    "    return speed, acc\r\n",
    "\r\n",
    "def get_features(x_data):\r\n",
    "    features = []\r\n",
    "    cols = x_data.columns.tolist()\r\n",
    "    #Calculate features (STD, Average, Max, Min, Median, Variance) for each data columns X Y Z \r\n",
    "    for k in cols:\r\n",
    "        features.append(x_data[k].std(ddof=0))\r\n",
    "        features.append(np.average(x_data[k]))\r\n",
    "        features.append(np.max(x_data[k]))\r\n",
    "        features.append(np.min(x_data[k]))\r\n",
    "        features.append(np.median(x_data[k]))        \r\n",
    "        features.append(np.var(x_data[k]))\r\n",
    "        fd = np.abs(fft(np.array(x_data[k])))**2\r\n",
    "        features.append(stats.skew(fd))\r\n",
    "        features.append(stats.kurtosis(fd))\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def dataloader(overlap, window_size, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(\"loading the data...\", end=\"\\t\")\r\n",
    "    data_list = []\r\n",
    "    file_lengths = {1: [], 2: [], 3: []}\r\n",
    "    files = tqdm(glob.glob(\"../TrainData/*/*/*.csv\")) if verbose else glob.glob(\"../TrainData/*/*/*.csv\")\r\n",
    "    for file in files:\r\n",
    "        tempdf = pd.read_csv(file)\r\n",
    "        segmented_data = segmentation(tempdf, overlap, window_size)\r\n",
    "        if len(segmented_data)>0:\r\n",
    "            person = segmented_data[0].reset_index(drop=True).loc[0, \"subject_id\"]\r\n",
    "            file_lengths[person].append(len(segmented_data))   \r\n",
    "        data_list.extend(segmented_data)\r\n",
    "    return data_list, file_lengths\r\n",
    "\r\n",
    "\r\n",
    "def feature_extractor(data_list, verbose=True):\r\n",
    "    if verbose:\r\n",
    "        print(f\"extracting the features...\", end=\"  \")\r\n",
    "    X, y = {1:[], 2:[], 3:[]}, {1:[], 2:[], 3:[]}\r\n",
    "    num_range = trange(0,len(data_list)) if verbose else range(0,len(data_list))\r\n",
    "    for j in num_range:\r\n",
    "        #extract only xyz columns\r\n",
    "        person = data_list[j].loc[0, \"subject_id\"]\r\n",
    "        x_data = data_list[j].drop(columns=[\"subject_id\",\"activity\"])\r\n",
    "        X[person].append(get_features(x_data))\r\n",
    "        y[person].append(data_list[j].iloc[0, -1])\r\n",
    "    return X, y\r\n",
    "\r\n",
    "\r\n",
    "def majority_voting(predictions, file_lengths):\r\n",
    "    filtered_predictions = []\r\n",
    "    index = 0\r\n",
    "    for length in file_lengths:\r\n",
    "        file_pred = predictions[index:index+length]\r\n",
    "        modes = mode(file_pred)\r\n",
    "        majority_choice = modes.mode[0]\r\n",
    "        filtered_predictions.extend([majority_choice]*length)\r\n",
    "        index += length\r\n",
    "    return filtered_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LOOCV_train_evaluate(model, overlap_rate, window_size, voting=True, n_repeats=1, verbose=True):\r\n",
    "    scores = []\r\n",
    "    data_list, file_lengths = dataloader(overlap_rate, window_size, verbose=verbose)\r\n",
    "    stream_list = []\r\n",
    "    for df in data_list:\r\n",
    "        stream_list.append(get_streams(df))\r\n",
    "    X, y = feature_extractor(stream_list, verbose=verbose)\r\n",
    "    num_range = trange(n_repeats) if verbose else range(n_repeats)\r\n",
    "    for _ in num_range:\r\n",
    "        for p1, p2, p3 in [(1,2,3), (2,3,1), (3,1,2)]:\r\n",
    "            X_test, y_test = X[p1], y[p1]\r\n",
    "            X_train = X[p2] + X[p3]\r\n",
    "            y_train = y[p2] + y[p3]\r\n",
    "            # print(f\"training model for person {p1}/3...\", end=\"\\t\")\r\n",
    "            model.fit(X_train, y_train)\r\n",
    "            pred = model.predict(X_test)\r\n",
    "            if voting:\r\n",
    "                filtered_pred = majority_voting(pred, file_lengths[p1])\r\n",
    "                scores.append(accuracy_score(y_test, filtered_pred))\r\n",
    "            else:\r\n",
    "                scores.append(accuracy_score(y_test, pred))\r\n",
    "    if verbose:\r\n",
    "        print(f\"\\nMean Score: {np.mean(scores)}\")\r\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def GridSearch(parameters, csvpath = \"..\", filename=\"gridCV_results\", n_repeats=7, verbose=False, progress=True):\r\n",
    "    score_df = pd.DataFrame({\"model\": [], \"window_size\": [], \"overlap_rate\": [], \"n_repeats\":[], \r\n",
    "                            \"avg_score\": [], \"scores\":[]})\r\n",
    "    models, window_sizes, overlap_rates = parameters[\"model\"], parameters[\"window_size\"], parameters[\"overlap_rate\"]\r\n",
    "    combinations = [(i,j,k) for i in models for j in window_sizes for k in overlap_rates]\r\n",
    "    if progress:\r\n",
    "        combinations = tqdm(combinations)\r\n",
    "    for combination in combinations:\r\n",
    "        model, overlap_rate, window_size = combination\r\n",
    "        scores = LOOCV_train_evaluate(model, overlap_rate, window_size, n_repeats=n_repeats, verbose=verbose)\r\n",
    "        score_df = score_df.append({\"model\": model.__str__(), \"window_size\": window_size, \r\n",
    "                    \"overlap_rate\": overlap_rate, \"n_repeats\": n_repeats, \r\n",
    "                    \"avg_score\": np.mean(scores), \"scores\": scores}, ignore_index=True)\r\n",
    "    savepath = f\"{csvpath}/{filename}_{str(datetime.now())[:-7]}.csv\".replace(\":\", \".\")\r\n",
    "    score_df.to_csv(savepath, index=False)\r\n",
    "    print(f\"result exported to: {savepath}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}